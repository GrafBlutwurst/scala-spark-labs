{"paragraphs":[{"text":"import org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\nimport org.apache.spark.ml.clustering.KMeans\n\nval fileName = \"/home/grafblutwurst/tmp/seeds_dataset.csv\"\nval delim = \",\"\n\n\nval df = spark.read.option(\"header\", true).option(\"inferSchema\", true).option(\"delim\", delim).csv(fileName)\n\n\nval Array(train, test) = df.randomSplit(Array(0.9, 0.1))\n\nval assembler = new VectorAssembler().\n  setInputCols(df.columns.toArray).\n  setOutputCol(\"features\")\n\nval scaler = new StandardScaler().\n  setInputCol(assembler.getOutputCol).\n  setOutputCol(\"scaledFeatures\").\n  setWithStd(true).\n  setWithMean(false)\nval pipeline = new Pipeline().\n  setStages(Array(assembler, scaler))\n\nval scalerModel = pipeline.fit(train)\n\nval data = scalerModel.transform(train)\nval dataTest = scalerModel.transform(test)\n\nval model = new KMeans().setK(3).setFeaturesCol(scaler.getOutputCol).fit(data)\n\nprintln(s\"With Set Sum of Squared Errors: ${model.computeCost(data)}\")\n\nprintln(s\"Cluter Centers\")\nmodel.clusterCenters.foreach(println)\n\nprintln(\"Test Predictions\")\nmodel.transform(dataTest).show","user":"admin","dateUpdated":"2019-02-09T16:47:04+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549730808184_1841878055","id":"20190209-164648_1183645036","dateCreated":"2019-02-09T16:46:48+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9030"}],"name":"Day4_ML/ClusteringExercise","id":"2E54WF29R","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}