{"paragraphs":[{"title":"Split Data into Training and Test","text":"val Array(trainingData, testData) = final_data.randomSplit(Array(0.7, 0.3))","user":"admin","dateUpdated":"2019-02-09T16:46:04+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549730721047_-1074744287","id":"20190209-164521_1811653806","dateCreated":"2019-02-09T16:45:21+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8065"},{"text":"import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.sql.DataFrame\n//Filename to use as well as the delimitter of the file\nval fileName = \"/home/grafblutwurst/tmp/clean_tweet.csv\"\nval delim = \",\"\n\n//load the data\nval df = spark.read.option(\"inferSchema\", true).option(\"header\", true).option(\"delim\", delim).csv(fileName)\n\n//clean dataset and restict to pos and neg class and map pos to 1, neg to 0\nval dfCleaned = df.na.drop.select($\"text\", $\"target\".alias(\"label\").cast(\"Double\"))\n\n//split into train and test dataset. we don't want to evaluate out model on the same data as we train it\nval Array(train, test) = dfCleaned.randomSplit(Array(0.9,0.1))\n\nval heuristicMap = Map(\n  \"happy\" -> 1,\n  \"sad\" -> -1\n)\n\ndef containsHeuristicEntry(s:String, fallBack:Double):Double = s.\n  split(\"\\\\s\").\n  map(w => heuristicMap.getOrElse(w, 0)).\n  sum match {\n    case i if i > 0 => 1d\n    case i if i < 0 => 0d\n    case _ => fallBack\n  }\n\nval heuristicUDF = udf[Double,String,Double](containsHeuristicEntry)\n\nval dfTest = \n  train.\n    withColumn(\"randomlabel\", when(rand() > 0.5, 1d).otherwise(0d)).\n    withColumn(\"heuristiclabel\", heuristicUDF($\"text\", $\"randomlabel\"))\n\nval tokenizer = new Tokenizer().\n  setInputCol(\"text\").\n  setOutputCol(\"words\")\nval hashingTF = new HashingTF().\n  setNumFeatures(1000).\n  setInputCol(tokenizer.getOutputCol).\n  setOutputCol(\"features\")\nval lr = new LogisticRegression().\n  setMaxIter(1000).\n  setRegParam(0.001)\nval pipeline = new Pipeline().\n  setStages(Array(tokenizer, hashingTF, lr))\n\nval model = pipeline.fit(train)\n\nval predictions = model.transform(dfTest) \n\ndef printMetricsForLabel(df:DataFrame, pred:String, label:String):Unit = {\n  val preds = df.select(col(pred),col(label)).as[(Double,Double)]\n\n  val precisionPerLabel = preds.groupByKey(_._1).flatMapGroups( \n    (k, iter) => {\n       val (correct, count) = iter.map(tpl => (if(tpl._1 == tpl._2) 1d else 0d) -> 1d).reduce( (tpl1, tpl2) => (tpl1._1 + tpl2._1) -> (tpl1._2 + tpl2._2))\n       List( k -> (correct / count))\n    }\n  ).toDF(\"label\", \"precision\")\n\n  val recallPerLabel = preds.groupByKey(_._2).flatMapGroups( \n    (k, iter) => {\n       val (correct, count) = iter.map(tpl => (if(tpl._1 == tpl._2) 1d else 0d) -> 1d).reduce( (tpl1, tpl2) => (tpl1._1 + tpl2._1) -> (tpl1._2 + tpl2._2))\n       List( k -> (correct / count))\n    }\n  ).toDF(\"label\", \"recall\")\n\n  val (correct, count) = preds.map(tpl => (if(tpl._1 == tpl._2) 1d else 0d) -> 1d).reduce( (tpl1, tpl2) => (tpl1._1 + tpl2._1) -> (tpl1._2 + tpl2._2))\n  \n  precisionPerLabel.show()\n  recallPerLabel.show()\n  println(s\"Overall Accuracy: $correct / $count = ${correct / count}\")\n}\n\n\n\nprintln(\"Random Performance\")\nprintMetricsForLabel(predictions, \"randomlabel\", \"label\")\n\nprintln(\"Heuristic Performance\")\nprintMetricsForLabel(predictions, \"heuristiclabel\", \"label\")\n\nprintln(\"LR Performance\")\nprintMetricsForLabel(predictions, \"prediction\", \"label\")","user":"admin","dateUpdated":"2019-02-09T16:46:34+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1549730768310_1753254209","id":"20190209-164608_1353438803","dateCreated":"2019-02-09T16:46:08+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:8066"}],"name":"Day4_ML/ClassificationExercise","id":"2E4132FPJ","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}